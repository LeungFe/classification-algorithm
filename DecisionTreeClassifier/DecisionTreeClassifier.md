# DecisionTreeClassifier

决策树又称为判定树，是运用于分类的一种树结构，其中的每个内部节点代表对某一属性的一次测试，每条边代表一个测试结果，叶节点代表某个类或类的分布。决策树的决策过程需要从决策树的根节点开始，待测数据与决策树中的特征节点进行比较，并按照比较结果选择选择下一比较分支，直到叶子节点作为最终的决策结果。

常见的决策树有ID3、C4.5和CART

ID3以信息增益度量属性选择（选择信息增益最大的属性作为分裂属性）、C4.5以信息增益率度量属性选择（选择信息增益率最大的属性作为分裂属性）、CART用gini系数度量属性选择（选择基尼系数最小的属性作为分裂属性、基尼系数越大，样本的不确定性越大）

# 1、ID3
1.1、计算公式

1）计算类别信息熵

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190416214659217.png)

类别信息熵表示所有样本中各类别出现的不确定性，熵越大、不确定性越大

2）计算属性信息熵

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190416214724891.png)

属性信息熵表示在某种属性的条件下，各类别出现的不确定性之和，属性信息熵越大，表示这个属性拥有的样本类别越不纯

3）计算信息增益

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190416214805528.png)

信息增益表示不确定性减少的程度，如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，也就是说，选择信息增益大的属性可以更快的完成分类任务，但如果每个属性中每种类别都只有一个样本，那么属性的信息熵为0，根据信息增益无法选择出有效的分类特征，C4.5选择信息增益率来对ID3进行改进。

1.2、ID3的缺点：

1、没有剪枝过程，为了去除过渡数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点；

2、信息增益的方法偏向选择具有大量值的属性，也就是说某个属性特征索取的不同值越多，那么越有可能作为分裂属性，这样是不合理的；

3、只可以处理离散分布的数据特征；

4、没有考虑缺失值的情况；
